{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "We begin where all ML use cases do: data engineering. In this section of the demo, we will utilize Snowpark's Python client-side Dataframe API to build an **ELT pipeline**.  We will extract the data from the source system (s3), load it into snowflake and add transformations to clean the data before analysis. \n",
    "\n",
    "The data engineer has been told that there is historical data going back to 2013 and new data will be made available at the end of each month. \n",
    "\n",
    "Input: Historical bulk data at `https://s3.amazonaws.com/tripdata/`. Incremental data to be loaded one month at a time.  \n",
    "Output: `trips` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {\n",
    "    \"connection_parameters\": {  \"user\": \"USER_SNOWPARK\",\n",
    "                                \"account\": \"pr47095.east-us-2.azure\",\n",
    "                                \"database\": \"SNOWPARK\",   \n",
    "                                \"role\": \"ACCOUNTADMIN\",\n",
    "                                \"schema\": \"LAB\",\n",
    "                                \"password\": \"Teste@123\"\n",
    "                             },\n",
    "    \"compute_parameters\" : {\"default_warehouse\": \"DEV_SMALL\",  \n",
    "                            \"task_warehouse\": \"DEV_SMALL\",  \n",
    "                            \"load_warehouse\": \"DEV_SMALL\",  \n",
    "                            \"fe_warehouse\": \"DEV_SMALL\",  \n",
    "                            \"train_warehouse\": \"DEV_SMALL\"  \n",
    "                            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load  credentials and connect to Snowflake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will utilize a simple json file to store our credentials. This should **never** be done in production and is for demo purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "session, state_dict = snowpark_connect('./include/state.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a stage for loading data to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict['load_stage_name']='LOAD_STAGE' \n",
    "state_dict['download_base_url']='https://s3.amazonaws.com/tripdata/'\n",
    "state_dict['trips_table_name']='TRIPS'\n",
    "state_dict['load_table_name'] = 'RAW_'\n",
    "\n",
    "import json\n",
    "with open('./include/state.json', 'w') as sdf:\n",
    "    json.dump(state_dict, sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_database(session, state_dict:dict, prestaged=False):\n",
    "    _ = session.sql('CREATE OR REPLACE DATABASE '+state_dict['connection_parameters']['database']).collect()\n",
    "    _ = session.sql('CREATE SCHEMA '+state_dict['connection_parameters']['schema']).collect() \n",
    "\n",
    "    if prestaged:\n",
    "        sql_cmd = 'CREATE OR REPLACE STAGE '+state_dict['load_stage_name']+\\\n",
    "                  ' url='+state_dict['connection_parameters']['download_base_url']\n",
    "        _ = session.sql(sql_cmd).collect()\n",
    "    else: \n",
    "        _ = session.sql('CREATE STAGE IF NOT EXISTS '+state_dict['load_stage_name']).collect()\n",
    "reset_database(session, state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of files to download and upload to stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#For files like 201306-citibike-tripdata.zip\n",
    "date_range1 = pd.period_range(start=datetime.strptime(\"201307\", \"%Y%m\"), \n",
    "                             end=datetime.strptime(\"201612\", \"%Y%m\"), \n",
    "                             freq='M').strftime(\"%Y%m\")\n",
    "file_name_end1 = '-citibike-tripdata.zip'\n",
    "files_to_download = [date+file_name_end1 for date in date_range1.to_list()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting in January 2017 Citibike changed the format of the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For files like 201701-citibike-tripdata.csv.zip\n",
    "date_range2 = pd.period_range(start=datetime.strptime(\"202201\", \"%Y%m\"), \n",
    "                             end=datetime.strptime(\"202209\", \"%Y%m\"), \n",
    "                             freq='M').strftime(\"%Y%m\")\n",
    "file_name_end2 = '-citibike-tripdata.csv.zip'\n",
    "files_to_download = files_to_download + [date+file_name_end2 for date in date_range2.to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For development purposes we will start with loading just a couple of files.  We will create a bulk load process afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_warehouse(state_dict['compute_parameters']['fe_warehouse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1_download_files = list()\n",
    "schema2_download_files = list()\n",
    "schema2_start_date = datetime.strptime('202101', \"%Y%m\")\n",
    "\n",
    "for file_name in files_to_download:\n",
    "    file_start_date = datetime.strptime(file_name.split(\"-\")[0], \"%Y%m\")\n",
    "    if file_start_date < schema2_start_date:\n",
    "        schema1_download_files.append(file_name)\n",
    "    else:\n",
    "        schema2_download_files.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1_download_files, schema2_download_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_load = \"pre-load\"\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(path_load)\n",
    "if not isExist:\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(path_load)\n",
    "   print(\"The new directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_preprocessed = \"pre-processed\"\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(path_preprocessed)\n",
    "if not isExist:\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(path_preprocessed)\n",
    "   print(\"The new directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_temp = \"temp\"\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(path_preprocessed)\n",
    "if not isExist:\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(path_preprocessed)\n",
    "   print(\"The new directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_load = \"pre-load\"\n",
    "for zip_file_name in schema1_download_files:\n",
    "    url = state_dict['download_base_url']+zip_file_name\n",
    "    print('Downloading: '+url)\n",
    "    r = requests.get(url)\n",
    "    #write file\n",
    "    open(zip_file_name,'wb').write(r.content)\n",
    "    #move file to pre-load\n",
    "    os.replace(zip_file_name, path_load+\"/\"+zip_file_name)\n",
    "# 26min - running..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_load = \"pre-load\"\n",
    "for zip_file_name in schema2_download_files:\n",
    "    url = state_dict['download_base_url']+zip_file_name\n",
    "    print('Downloading: '+url)\n",
    "    r = requests.get(url)\n",
    "    #write file\n",
    "    open(zip_file_name,'wb').write(r.content)\n",
    "    #move file to pre-load\n",
    "    os.replace(zip_file_name, path_load+\"/\"+zip_file_name)\n",
    "# 8min - running..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import zipfile\n",
    "#Change Zip to Gzip\n",
    "path_temp = \"temp\\\\\"\n",
    "path_load = \"pre-load\\\\\"\n",
    "path_preprocessed = \"pre-processed\\\\\"\n",
    "for zip_file_name in os.listdir(path_load):\n",
    "    #zip_file_name = \"201306-citibike-tripdata.zip\"\n",
    "\n",
    "    zip = zipfile.ZipFile( path_load + zip_file_name )\n",
    "    csv_file_name = zip.namelist()[0]\n",
    "    gz_file_name = csv_file_name #NO EXTENSION BECAUSE ON PUT INTO ON SNOWFLAKE IS SETTING AUTOMATIC\n",
    "    zip.extract( csv_file_name, path_temp )\n",
    "    print ( \"extracting file: \" + csv_file_name + \" to \" + path_temp )\n",
    "\n",
    "    file = open( path_temp + csv_file_name,\"rb\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    # bindata = bytearray( data )\n",
    "    # with gzip.open( path_temp + gz_file_name, 'wb' ) as f:\n",
    "    #     f.write( bindata )\n",
    "    # print ( \"compress gz file: \" + gz_file_name )\n",
    "    os.replace( path_temp + csv_file_name, path_preprocessed + csv_file_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1_load_stage = state_dict['load_stage_name']+'/schema1/'\n",
    "schema2_load_stage = state_dict['load_stage_name']+'/schema2/'\n",
    "\n",
    "schema1_files_to_load = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_preprocessed = \"pre-processed\\\\\"\n",
    "\n",
    "for file_name in os.listdir(path_preprocessed):\n",
    "    path_file_name = path_preprocessed + file_name\n",
    "    print('Putting '+path_file_name+' to stage: '+schema1_load_stage)\n",
    "    session.file.put(local_file_name=path_file_name, \n",
    "                     stage_location=schema1_load_stage, \n",
    "                     source_compression='NONE', \n",
    "                     overwrite=True)\n",
    "    schema1_files_to_load.append(path_file_name)\n",
    "    os.remove(path_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_preprocessed = \"pre-processed\\\\\"\n",
    "\n",
    "for file_name in os.listdir(path_preprocessed):\n",
    "    path_file_name = path_preprocessed + file_name\n",
    "    print('Putting '+path_file_name+' to stage: '+schema2_load_stage)\n",
    "    session.file.put(local_file_name=path_file_name, \n",
    "                     stage_location=schema2_load_stage, \n",
    "                     source_compression='NONE', \n",
    "                     overwrite=True)\n",
    "    schema1_files_to_load.append(path_file_name)\n",
    "    os.remove(path_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='load_stage/schema1/2013-07 - Citi Bike trip data.csv.gz', size=24672928, md5='f7271ba1db464073b34aba2642aba3fd', last_modified='Sun, 6 Nov 2022 19:08:13 GMT'),\n",
       " Row(name='load_stage/schema1/2013-08 - Citi Bike trip data.csv.gz', size=29211040, md5='bf1911dedffa304e817ad310ac502e49', last_modified='Sun, 6 Nov 2022 19:10:11 GMT'),\n",
       " Row(name='load_stage/schema1/2013-09 - Citi Bike trip data.csv.gz', size=30174160, md5='5e31275903b9baaab143639d12750116', last_modified='Sun, 6 Nov 2022 19:11:45 GMT'),\n",
       " Row(name='load_stage/schema1/2013-10 - Citi Bike trip data.csv.gz', size=30070736, md5='cfa2e1607cec91f8aa9430331a9ae26b', last_modified='Sun, 6 Nov 2022 19:14:21 GMT'),\n",
       " Row(name='load_stage/schema1/2013-11 - Citi Bike trip data.csv.gz', size=19675104, md5='ef9d8c6a9b4051e0af9babad200f4d80', last_modified='Sun, 6 Nov 2022 19:15:42 GMT'),\n",
       " Row(name='load_stage/schema1/2013-12 - Citi Bike trip data.csv.gz', size=13043312, md5='cead02ae1cd97c446b3fe4efb0d07ce5', last_modified='Sun, 6 Nov 2022 19:16:50 GMT'),\n",
       " Row(name='load_stage/schema1/201306-citibike-tripdata.csv.gz', size=16218896, md5='c6a2118f2ef6c885a1a4e4ead742d229', last_modified='Sun, 6 Nov 2022 19:20:10 GMT'),\n",
       " Row(name='load_stage/schema1/2014-01 - Citi Bike trip data.csv.gz', size=8840112, md5='e420bc2dcb97b04538402f739cb38842', last_modified='Sun, 6 Nov 2022 19:21:55 GMT'),\n",
       " Row(name='load_stage/schema1/2014-02 - Citi Bike trip data.csv.gz', size=6609536, md5='63c967d79dcceca9f04ea2c10780c029', last_modified='Sun, 6 Nov 2022 19:23:11 GMT'),\n",
       " Row(name='load_stage/schema1/2014-03 - Citi Bike trip data.csv.gz', size=12878336, md5='c6ebcad6ad2eddf50076ae8be214a5bc', last_modified='Sun, 6 Nov 2022 19:26:03 GMT'),\n",
       " Row(name='load_stage/schema1/2014-04 - Citi Bike trip data.csv.gz', size=19492432, md5='59e5c121cf72b848de2897cf6f0c217f', last_modified='Sun, 6 Nov 2022 19:32:28 GMT'),\n",
       " Row(name='load_stage/schema1/2014-05 - Citi Bike trip data.csv.gz', size=25113984, md5='46f14fc930b2638adf0307dbb5bd1d61', last_modified='Sun, 6 Nov 2022 19:40:08 GMT'),\n",
       " Row(name='load_stage/schema1/2014-06 - Citi Bike trip data.csv.gz', size=27218384, md5='fa4bb5e843b06ed735ac99908d68ecda', last_modified='Sun, 6 Nov 2022 20:02:48 GMT'),\n",
       " Row(name='load_stage/schema1/2014-07 - Citi Bike trip data.csv.gz', size=28113120, md5='e0f5d4f872429f0023488422a6afe6da', last_modified='Sun, 6 Nov 2022 20:11:19 GMT'),\n",
       " Row(name='load_stage/schema1/2014-08 - Citi Bike trip data.csv.gz', size=27866896, md5='6f9474053b9d65a9652b30e1e194fe79', last_modified='Sun, 6 Nov 2022 20:17:19 GMT'),\n",
       " Row(name='load_stage/schema1/201409-citibike-tripdata.csv.gz', size=27511440, md5='a02c935dd42929eef3b55b0124426a16', last_modified='Sun, 6 Nov 2022 20:22:28 GMT'),\n",
       " Row(name='load_stage/schema1/201410-citibike-tripdata.csv.gz', size=23787024, md5='f88da12d838605b1d5ebe387d11b01e0', last_modified='Sun, 6 Nov 2022 20:28:52 GMT'),\n",
       " Row(name='load_stage/schema1/201411-citibike-tripdata.csv.gz', size=15312112, md5='363ff3be24d8346e006d55a8034d2f61', last_modified='Sun, 6 Nov 2022 20:32:40 GMT'),\n",
       " Row(name='load_stage/schema1/201412-citibike-tripdata.csv.gz', size=11583440, md5='0f923926095d82f121f2119c14830173', last_modified='Sun, 6 Nov 2022 20:35:35 GMT'),\n",
       " Row(name='load_stage/schema1/201501-citibike-tripdata.csv.gz', size=6615152, md5='b709f758d706e5895a0b81811dc2ec64', last_modified='Sun, 6 Nov 2022 20:37:27 GMT'),\n",
       " Row(name='load_stage/schema1/201502-citibike-tripdata.csv.gz', size=4563824, md5='3913263195e5015f4b53efaff24a7769', last_modified='Sun, 6 Nov 2022 20:38:52 GMT'),\n",
       " Row(name='load_stage/schema1/201503-citibike-tripdata.csv.gz', size=7943824, md5='ba83a307bebb68707d3517558849c3aa', last_modified='Sun, 6 Nov 2022 20:41:32 GMT'),\n",
       " Row(name='load_stage/schema1/201504-citibike-tripdata.csv.gz', size=18896112, md5='770b8db31658ddb14516bfecd635795f', last_modified='Sun, 6 Nov 2022 20:45:11 GMT'),\n",
       " Row(name='load_stage/schema1/201505-citibike-tripdata.csv.gz', size=27609648, md5='7fdb36a92c9c0d6f986b41622e0ec40b', last_modified='Sun, 6 Nov 2022 21:01:08 GMT'),\n",
       " Row(name='load_stage/schema1/201506-citibike-tripdata.csv.gz', size=21576224, md5='d1ddfd22e4204a70a5d38fc9abb41099', last_modified='Sun, 6 Nov 2022 21:03:15 GMT'),\n",
       " Row(name='load_stage/schema1/201507-citibike-tripdata.csv.gz', size=31388656, md5='51295270ace9d846ca0291ee71e036d2', last_modified='Sun, 6 Nov 2022 21:07:12 GMT'),\n",
       " Row(name='load_stage/schema1/201508-citibike-tripdata.csv.gz', size=34635024, md5='ef47c7806c865b4c964ef021d9b2c242', last_modified='Sun, 6 Nov 2022 21:14:18 GMT'),\n",
       " Row(name='load_stage/schema1/201509-citibike-tripdata.csv.gz', size=39864384, md5='caa5f279a9c557a6ffbc511189aec30f', last_modified='Sun, 6 Nov 2022 21:25:10 GMT'),\n",
       " Row(name='load_stage/schema1/201510-citibike-tripdata.csv.gz', size=38265936, md5='525cb92b00826ac6cb00b43c2c0a9ea7', last_modified='Sun, 6 Nov 2022 21:28:57 GMT'),\n",
       " Row(name='load_stage/schema1/201511-citibike-tripdata.csv.gz', size=31452800, md5='0ae93d9c9fcc5c02af94ac8c0d3d66a5', last_modified='Sun, 6 Nov 2022 21:32:15 GMT'),\n",
       " Row(name='load_stage/schema1/201512-citibike-tripdata.csv.gz', size=21186720, md5='51a8ff7474f915fc638b528647abe52d', last_modified='Sun, 6 Nov 2022 21:33:50 GMT'),\n",
       " Row(name='load_stage/schema1/201601-citibike-tripdata.csv.gz', size=16318928, md5='f2e4df60861cea2b760f58442142a1cf', last_modified='Sun, 6 Nov 2022 21:35:02 GMT'),\n",
       " Row(name='load_stage/schema1/201602-citibike-tripdata.csv.gz', size=18052624, md5='f9c810cd9c1c716d6ac9277083ee3339', last_modified='Sun, 6 Nov 2022 21:36:10 GMT'),\n",
       " Row(name='load_stage/schema1/201603-citibike-tripdata.csv.gz', size=24381504, md5='0cf7ab427608c7eac72c5b9d0fe1d68c', last_modified='Sun, 6 Nov 2022 21:37:26 GMT'),\n",
       " Row(name='load_stage/schema1/201604-citibike-tripdata.csv.gz', size=32547952, md5='ac6129079873b3051801435c51c211cb', last_modified='Sun, 6 Nov 2022 21:38:51 GMT'),\n",
       " Row(name='load_stage/schema1/201605-citibike-tripdata.csv.gz', size=38897360, md5='aca89f6cacef119e4569deebe3a61df8', last_modified='Sun, 6 Nov 2022 21:41:03 GMT'),\n",
       " Row(name='load_stage/schema1/201606-citibike-tripdata.csv.gz', size=47103520, md5='d3a1da1955e4e9bae62849e1f2845754', last_modified='Sun, 6 Nov 2022 21:44:36 GMT'),\n",
       " Row(name='load_stage/schema1/201607-citibike-tripdata.csv.gz', size=44752240, md5='40310c69d17d29a67e13b198430993a8', last_modified='Mon, 7 Nov 2022 04:35:11 GMT'),\n",
       " Row(name='load_stage/schema1/201608-citibike-tripdata.csv.gz', size=51406624, md5='730bf55c7f6e9249486a4d594d750521', last_modified='Mon, 7 Nov 2022 04:37:19 GMT'),\n",
       " Row(name='load_stage/schema1/201609-citibike-tripdata.csv.gz', size=55865328, md5='fddfd609dec530ca5e4efcfce6f2cd42', last_modified='Mon, 7 Nov 2022 04:38:58 GMT'),\n",
       " Row(name='load_stage/schema1/201610-citibike-tripdata.csv.gz', size=50200176, md5='208f660fc60fe6c767a18c707c06e133', last_modified='Mon, 7 Nov 2022 04:40:10 GMT'),\n",
       " Row(name='load_stage/schema1/201611-citibike-tripdata.csv.gz', size=38245040, md5='a1fca54de5ff4143165777722136bc09', last_modified='Mon, 7 Nov 2022 04:41:17 GMT'),\n",
       " Row(name='load_stage/schema1/201612-citibike-tripdata.csv.gz', size=26144736, md5='2077ac6f5ad2794350e0d26e5a99ab7c', last_modified='Mon, 7 Nov 2022 04:42:03 GMT'),\n",
       " Row(name='load_stage/schema1/201701-citibike-tripdata.csv.gz', size=23432912, md5='856e7e8a587338b90c40b89b572d2dcb', last_modified='Mon, 7 Nov 2022 04:42:34 GMT'),\n",
       " Row(name='load_stage/schema1/201702-citibike-tripdata.csv.gz', size=25454784, md5='3f55f902d80fcea97d27caeb428ff594', last_modified='Mon, 7 Nov 2022 04:43:04 GMT'),\n",
       " Row(name='load_stage/schema1/201703-citibike-tripdata.csv.gz', size=23354608, md5='a2dcf56a239c73c514bad0a849753c64', last_modified='Mon, 7 Nov 2022 04:43:33 GMT'),\n",
       " Row(name='load_stage/schema1/201704-citibike-tripdata.csv.gz', size=43605584, md5='e71379ca9e42bf91ae2e7768a6d8cb55', last_modified='Mon, 7 Nov 2022 04:44:53 GMT'),\n",
       " Row(name='load_stage/schema1/201705-citibike-tripdata.csv.gz', size=50948736, md5='4a2d4a3e241f1098d21a485dc269de4b', last_modified='Mon, 7 Nov 2022 04:46:25 GMT'),\n",
       " Row(name='load_stage/schema1/201706-citibike-tripdata.csv.gz', size=58428704, md5='8e133683af3f8cf915676f13b6e2bb52', last_modified='Mon, 7 Nov 2022 04:48:06 GMT'),\n",
       " Row(name='load_stage/schema1/201707-citibike-tripdata.csv.gz', size=58596688, md5='f9ec427ea69edb46c3df9e2e3fc6077e', last_modified='Mon, 7 Nov 2022 04:49:23 GMT'),\n",
       " Row(name='load_stage/schema1/201708-citibike-tripdata.csv.gz', size=61224640, md5='b9051612a6dba884b041759c5a3b3f7f', last_modified='Mon, 7 Nov 2022 04:50:54 GMT'),\n",
       " Row(name='load_stage/schema1/201709-citibike-tripdata.csv.gz', size=63734224, md5='c4151575877195465fae5963ecbdfd54', last_modified='Mon, 7 Nov 2022 04:52:19 GMT'),\n",
       " Row(name='load_stage/schema1/201710-citibike-tripdata.csv.gz', size=65419808, md5='d2d67cf4f061910b50e415792c336140', last_modified='Mon, 7 Nov 2022 04:53:25 GMT'),\n",
       " Row(name='load_stage/schema1/201711-citibike-tripdata.csv.gz', size=46221920, md5='3f4f9800fd3a871560430ad46998d457', last_modified='Mon, 7 Nov 2022 04:54:18 GMT'),\n",
       " Row(name='load_stage/schema1/201712-citibike-tripdata.csv.gz', size=31059216, md5='8a6fd850a8634a33dffbdd0bb85ed468', last_modified='Mon, 7 Nov 2022 04:55:23 GMT'),\n",
       " Row(name='load_stage/schema1/201801-citibike-tripdata.csv.gz', size=22322048, md5='e3cf894aeccd274e6847be1173dc19e6', last_modified='Mon, 7 Nov 2022 04:55:49 GMT'),\n",
       " Row(name='load_stage/schema1/201802-citibike-tripdata.csv.gz', size=26349760, md5='9047a327d4af25c3ab93a23164a3e41d', last_modified='Mon, 7 Nov 2022 04:56:29 GMT'),\n",
       " Row(name='load_stage/schema1/201803-citibike-tripdata.csv.gz', size=30491536, md5='e81a37ee682240bfa1110a28a1c85da1', last_modified='Mon, 7 Nov 2022 04:57:46 GMT'),\n",
       " Row(name='load_stage/schema1/201804-citibike-tripdata.csv.gz', size=40998576, md5='8354b3477c98fbedc92f694e26612a4d', last_modified='Mon, 7 Nov 2022 04:59:01 GMT'),\n",
       " Row(name='load_stage/schema1/201805-citibike-tripdata.csv.gz', size=57863792, md5='dcb8f665b9ffe39ea5c2858b29ecbd17', last_modified='Mon, 7 Nov 2022 05:01:09 GMT'),\n",
       " Row(name='load_stage/schema1/201806-citibike-tripdata.csv.gz', size=61944592, md5='6e69c152059727268eaf234c5df78798', last_modified='Mon, 7 Nov 2022 05:03:22 GMT'),\n",
       " Row(name='load_stage/schema1/201807-citibike-tripdata.csv.gz', size=60480192, md5='2da2f21de5ffb6a66b4808b36e3b4ccb', last_modified='Mon, 7 Nov 2022 05:05:15 GMT'),\n",
       " Row(name='load_stage/schema1/201808-citibike-tripdata.csv.gz', size=81573840, md5='a775ed4f8dc757491ba49b84fee3855b', last_modified='Mon, 7 Nov 2022 05:07:25 GMT'),\n",
       " Row(name='load_stage/schema1/201809-citibike-tripdata.csv.gz', size=77480864, md5='d45d75d0edc345c919283038e3d70614', last_modified='Mon, 7 Nov 2022 05:09:01 GMT'),\n",
       " Row(name='load_stage/schema1/201810-citibike-tripdata.csv.gz', size=77367648, md5='0da926c5f6da06208d346e82a89363b5', last_modified='Mon, 7 Nov 2022 05:10:28 GMT'),\n",
       " Row(name='load_stage/schema1/201811-citibike-tripdata.csv.gz', size=51986992, md5='53e8970a31de25bc9a39116afd4374ef', last_modified='Mon, 7 Nov 2022 05:11:41 GMT'),\n",
       " Row(name='load_stage/schema1/201812-citibike-tripdata.csv.gz', size=42201568, md5='aa5a5b9e446c5a8a822a1a6c92331c54', last_modified='Mon, 7 Nov 2022 05:13:11 GMT'),\n",
       " Row(name='load_stage/schema1/201901-citibike-tripdata.csv.gz', size=40120448, md5='19a24af00bbed66b46fe1bc41179106a', last_modified='Mon, 7 Nov 2022 05:14:05 GMT'),\n",
       " Row(name='load_stage/schema1/201902-citibike-tripdata.csv.gz', size=39153104, md5='31f8da1c78730a78cb9558712ae48a44', last_modified='Mon, 7 Nov 2022 05:15:09 GMT'),\n",
       " Row(name='load_stage/schema1/201903-citibike-tripdata.csv.gz', size=55354704, md5='43dbe92ec038def59441724c7508f4db', last_modified='Mon, 7 Nov 2022 05:16:19 GMT'),\n",
       " Row(name='load_stage/schema1/201904-citibike-tripdata.csv.gz', size=73262960, md5='939f3821923c56b15c6d5c09a94b5747', last_modified='Mon, 7 Nov 2022 05:18:06 GMT'),\n",
       " Row(name='load_stage/schema1/201905-citibike-tripdata.csv.gz', size=79499312, md5='e45295254efefa2b323f15606f0c33ac', last_modified='Mon, 7 Nov 2022 05:19:36 GMT'),\n",
       " Row(name='load_stage/schema1/201906-citibike-tripdata.csv.gz', size=88125872, md5='7b2cf6d7d4970aa0a15f8bfc25213712', last_modified='Mon, 7 Nov 2022 05:21:57 GMT'),\n",
       " Row(name='load_stage/schema1/201907-citibike-tripdata.csv.gz', size=90335008, md5='47f7f8e93a32dd1bbfad91a958228440', last_modified='Mon, 7 Nov 2022 05:24:04 GMT'),\n",
       " Row(name='load_stage/schema1/201908-citibike-tripdata.csv.gz', size=96773904, md5='d1bb7ced6b490e87237f19ed5ac54fb2', last_modified='Mon, 7 Nov 2022 05:26:09 GMT'),\n",
       " Row(name='load_stage/schema1/201909-citibike-tripdata.csv.gz', size=100800048, md5='c5b5e020f7e2c02e861acae7649ef106', last_modified='Mon, 7 Nov 2022 05:28:51 GMT'),\n",
       " Row(name='load_stage/schema1/201910-citibike-tripdata.csv.gz', size=86282800, md5='af6dff8a1b11498f2d273a6fa3ab5ec9', last_modified='Mon, 7 Nov 2022 05:30:55 GMT'),\n",
       " Row(name='load_stage/schema1/201911-citibike-tripdata.csv.gz', size=61138624, md5='931462669a5318622a84a36775126e4e', last_modified='Mon, 7 Nov 2022 05:32:25 GMT'),\n",
       " Row(name='load_stage/schema1/201912-citibike-tripdata.csv.gz', size=39760432, md5='5148fe3ac779876452e95b70a65006de', last_modified='Mon, 7 Nov 2022 05:33:53 GMT'),\n",
       " Row(name='load_stage/schema1/202001-citibike-tripdata.csv.gz', size=51733952, md5='1048a29055201d3c8b000e5973343b5d', last_modified='Mon, 7 Nov 2022 05:35:20 GMT'),\n",
       " Row(name='load_stage/schema1/202002-citibike-tripdata.csv.gz', size=47915936, md5='3775195e963b9db4c5eb203aa50d4b2c', last_modified='Mon, 7 Nov 2022 05:36:23 GMT'),\n",
       " Row(name='load_stage/schema1/202003-citibike-tripdata.csv.gz', size=45190800, md5='ee4ed9b059988598391b7826bdfba10b', last_modified='Mon, 7 Nov 2022 05:37:51 GMT'),\n",
       " Row(name='load_stage/schema1/202004-citibike-tripdata.csv.gz', size=29206800, md5='eabc1eb87dc2296cdfc179aa35fb0606', last_modified='Mon, 7 Nov 2022 05:38:37 GMT'),\n",
       " Row(name='load_stage/schema1/202005-citibike-tripdata.csv.gz', size=63331616, md5='847469e8295d28cea7a1180e0836b465', last_modified='Mon, 7 Nov 2022 05:40:16 GMT'),\n",
       " Row(name='load_stage/schema1/202006-citibike-tripdata.csv.gz', size=80800432, md5='ae0f92be9407270eb0ae96bc04c571d4', last_modified='Mon, 7 Nov 2022 05:41:57 GMT'),\n",
       " Row(name='load_stage/schema1/202007-citibike-tripdata.csv.gz', size=91177056, md5='86a0ae4290a7d617e1cb8bf4bf291edf', last_modified='Mon, 7 Nov 2022 05:43:51 GMT'),\n",
       " Row(name='load_stage/schema1/202008-citibike-tripdata.csv.gz', size=101033296, md5='c9d8c7bb870d12e99700ff85c64fc213', last_modified='Mon, 7 Nov 2022 05:46:40 GMT'),\n",
       " Row(name='load_stage/schema1/202009-citibike-tripdata.csv.gz', size=108492384, md5='4f9fdde542e2b8942c973716c5dd2247', last_modified='Mon, 7 Nov 2022 05:49:04 GMT'),\n",
       " Row(name='load_stage/schema1/202010-citibike-tripdata.csv.gz', size=98721872, md5='17e30489e5542c82c252a12dc23751b4', last_modified='Mon, 7 Nov 2022 05:50:33 GMT'),\n",
       " Row(name='load_stage/schema1/202011-citibike-tripdata.csv.gz', size=76289872, md5='f89aff6286c182612bc1d4d9393fc79c', last_modified='Mon, 7 Nov 2022 05:53:34 GMT'),\n",
       " Row(name='load_stage/schema1/202012-citibike-tripdata.csv.gz', size=48051136, md5='bfee73e020d056d0be5594d28bd8ee09', last_modified='Mon, 7 Nov 2022 05:55:20 GMT'),\n",
       " Row(name='load_stage/schema1/202101-citibike-tripdata.csv.gz', size=48607520, md5='a25d3133e6f46342acc92489794d46b1', last_modified='Mon, 7 Nov 2022 05:56:04 GMT'),\n",
       " Row(name='load_stage/schema1/202102-citibike-tripdata.csv.gz', size=20663872, md5='415799a6ed4d826457b0bde8637a46db', last_modified='Mon, 7 Nov 2022 05:56:33 GMT'),\n",
       " Row(name='load_stage/schema1/202103-citibike-tripdata.csv.gz', size=49356752, md5='b691287be26765931e19fc941d4a4a64', last_modified='Mon, 7 Nov 2022 05:58:09 GMT'),\n",
       " Row(name='load_stage/schema1/202104-citibike-tripdata.csv.gz', size=63205328, md5='a4063576603be558eec1e6f8ff66ef99', last_modified='Mon, 7 Nov 2022 05:59:53 GMT'),\n",
       " Row(name='load_stage/schema1/202105-citibike-tripdata.csv.gz', size=77263792, md5='949ee985909926fe076081258b719b8f', last_modified='Mon, 7 Nov 2022 06:01:45 GMT'),\n",
       " Row(name='load_stage/schema1/202106-citibike-tripdata.csv.gz', size=105998928, md5='b6ca1359f75c1a564f4d0b7ce675ff1b', last_modified='Mon, 7 Nov 2022 06:04:59 GMT'),\n",
       " Row(name='load_stage/schema1/202107-citibike-tripdata.csv.gz', size=96633504, md5='a165ce7c43f1315c984e49e80eb5bca7', last_modified='Mon, 7 Nov 2022 06:08:12 GMT'),\n",
       " Row(name='load_stage/schema1/202108-citibike-tripdata.csv.gz', size=106076720, md5='5042668baaae2cbf166eda5dea596891', last_modified='Mon, 7 Nov 2022 06:17:38 GMT'),\n",
       " Row(name='load_stage/schema1/202109-citibike-tripdata.csv.gz', size=108202944, md5='16f10a1ececc44168a0356c29ad5dcb2', last_modified='Mon, 7 Nov 2022 06:25:51 GMT'),\n",
       " Row(name='load_stage/schema1/202110-citibike-tripdata.csv.gz', size=94387120, md5='29a8d8e961921c5de113b662e2bfd86c', last_modified='Mon, 7 Nov 2022 06:32:51 GMT'),\n",
       " Row(name='load_stage/schema1/202111-citibike-tripdata.csv.gz', size=69510944, md5='c4826c671e146a164dd0694f846fb36c', last_modified='Mon, 7 Nov 2022 06:37:53 GMT'),\n",
       " Row(name='load_stage/schema1/202112-citibike-tripdata.csv.gz', size=57392960, md5='644d037c8ff0a4bd8f0bdfbdc5e91e47', last_modified='Mon, 7 Nov 2022 06:41:50 GMT'),\n",
       " Row(name='load_stage/schema1/202201-citibike-tripdata.csv.gz', size=34861248, md5='79b602dda888081148ccb4c10c4a2a51', last_modified='Mon, 7 Nov 2022 06:43:46 GMT'),\n",
       " Row(name='load_stage/schema1/202202-citibike-tripdata.csv.gz', size=42541456, md5='bfe364ac10c0601572ea31fa91d3142e', last_modified='Mon, 7 Nov 2022 06:47:25 GMT'),\n",
       " Row(name='load_stage/schema1/202203-citibike-tripdata.csv.gz', size=65095248, md5='133453d0274c96901ceb83cb1bc1a707', last_modified='Mon, 7 Nov 2022 06:51:47 GMT'),\n",
       " Row(name='load_stage/schema1/202204-citibike-tripdata.csv.gz', size=78992656, md5='5825a54ed90eaf12338e62faea5b7646', last_modified='Mon, 7 Nov 2022 06:56:16 GMT'),\n",
       " Row(name='load_stage/schema1/202205-citibike-tripdata.csv.gz', size=100376384, md5='d1364e29d138f848228fbc944ef8b6d9', last_modified='Mon, 7 Nov 2022 07:02:26 GMT')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"list @\"+state_dict['load_stage_name']+\" pattern='.*20.*[.]gz'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load: \n",
    "Load raw as all string type.  We will fix data types in the transform stage.\n",
    "\n",
    "There are two schema types so we will create two ingest tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upper case fields are common to both schemas.\n",
    "#Schema from 2013 to 2021\n",
    "load_schema1 = T.StructType([T.StructField(\"tripduration\", T.StringType()),\n",
    "                             T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"bike_id\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType()), \n",
    "                             T.StructField(\"birth_year\", T.StringType()),\n",
    "                             T.StructField(\"gender\", T.StringType())])\n",
    "\n",
    "#starting in February 2021 the schema changed\n",
    "load_schema2 = T.StructType([T.StructField(\"ride_id\", T.StringType()), \n",
    "                             T.StructField(\"rideable_type\", T.StringType()), \n",
    "                             T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType())])\n",
    "\n",
    "trips_table_schema = T.StructType([T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.create_dataframe([[None]*len(load_schema1.names)], schema=load_schema1)\\\n",
    "       .na.drop()\\\n",
    "       .write\\\n",
    "       .save_as_table(state_dict['load_table_name']+'schema1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.create_dataframe([[None]*len(load_schema2.names)], schema=load_schema2)\\\n",
    "       .na.drop()\\\n",
    "       .write\\\n",
    "       .save_as_table(state_dict['load_table_name']+'schema2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load schema1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to execute query [queryID: 01a8268d-0a03-dd4e-0002-14160058205e]  COPY  INTO RAW_schema1 FROM @LOAD_STAGE/schema1/ PATTERN  = '.*2021.*[.]gz' FILE_FORMAT  = ( TYPE  = csv  FIELD_OPTIONALLY_ENCLOSED_BY = '\"' skip_header = 1  )\n",
      "100080 (22000): Number of columns in file (13) does not match that of the corresponding table (15), use file format option error_on_column_count_mismatch=false to ignore this error\n",
      "  File 'schema1/202102-citibike-tripdata.csv.gz', line 3, character 1\n",
      "  Row 1 starts at line 2, column \"RAW_SCHEMA1\"[\"USERTYPE\":13]\n",
      "  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client.\n"
     ]
    },
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1304): 100080 (22000): Number of columns in file (13) does not match that of the corresponding table (15), use file format option error_on_column_count_mismatch=false to ignore this error\n  File 'schema1/202102-citibike-tripdata.csv.gz', line 3, character 1\n  Row 1 starts at line 2, column \"RAW_SCHEMA1\"[\"USERTYPE\":13]\n  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Rodrigo\\Desktop\\Desk\\GIT\\snowpark-Citibike\\01_Data_Engineering.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m csv_file_format_options \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mFIELD_OPTIONALLY_ENCLOSED_BY\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mskip_header\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m loaddf \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mSKIP_HEADER\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                      \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mFIELD_OPTIONALLY_ENCLOSED_BY\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\\042\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                      \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mCOMPRESSION\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mGZIP\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                      \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mNULL_IF\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mN\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                      \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mNULL_IF\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mNULL\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                      \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mpattern\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.*2021.*[.]gz\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                      \u001b[39m.\u001b[39;49mschema(load_schema1)\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                      \u001b[39m.\u001b[39;49mcsv(\u001b[39m'\u001b[39;49m\u001b[39m@\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mschema1_load_stage)\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                      \u001b[39m.\u001b[39;49mcopy_into_table(state_dict[\u001b[39m'\u001b[39;49m\u001b[39mload_table_name\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mschema1\u001b[39;49m\u001b[39m'\u001b[39;49m), \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rodrigo/Desktop/Desk/GIT/snowpark-Citibike/01_Data_Engineering.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                       format_type_options\u001b[39m=\u001b[39;49mcsv_file_format_options)\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\_internal\\telemetry.py:133\u001b[0m, in \u001b[0;36mdf_collect_api_telemetry.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    132\u001b[0m     \u001b[39mwith\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_session\u001b[39m.\u001b[39mquery_history() \u001b[39mas\u001b[39;00m query_history:\n\u001b[1;32m--> 133\u001b[0m         result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    134\u001b[0m     plan \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_select_statement \u001b[39mor\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_plan\n\u001b[0;32m    135\u001b[0m     api_calls \u001b[39m=\u001b[39m [\n\u001b[0;32m    136\u001b[0m         \u001b[39m*\u001b[39mplan\u001b[39m.\u001b[39mapi_calls,\n\u001b[0;32m    137\u001b[0m         {TelemetryField\u001b[39m.\u001b[39mNAME\u001b[39m.\u001b[39mvalue: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataFrame.\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    138\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\dataframe.py:2550\u001b[0m, in \u001b[0;36mDataFrame.copy_into_table\u001b[1;34m(self, table_name, files, pattern, validation_mode, target_columns, transformations, format_type_options, statement_params, **copy_options)\u001b[0m\n\u001b[0;32m   2537\u001b[0m normalized_column_names \u001b[39m=\u001b[39m (\n\u001b[0;32m   2538\u001b[0m     [quote_name(col_name) \u001b[39mfor\u001b[39;00m col_name \u001b[39min\u001b[39;00m target_columns]\n\u001b[0;32m   2539\u001b[0m     \u001b[39mif\u001b[39;00m target_columns\n\u001b[0;32m   2540\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2541\u001b[0m )\n\u001b[0;32m   2542\u001b[0m transformation_exps \u001b[39m=\u001b[39m (\n\u001b[0;32m   2543\u001b[0m     [\n\u001b[0;32m   2544\u001b[0m         column\u001b[39m.\u001b[39m_expression \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(column, Column) \u001b[39melse\u001b[39;00m column\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2548\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m )\n\u001b[1;32m-> 2550\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\n\u001b[0;32m   2551\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session,\n\u001b[0;32m   2552\u001b[0m     CopyIntoTableNode(\n\u001b[0;32m   2553\u001b[0m         full_table_name,\n\u001b[0;32m   2554\u001b[0m         file_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49m_file_path,\n\u001b[0;32m   2555\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m   2556\u001b[0m         file_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49m_file_type,\n\u001b[0;32m   2557\u001b[0m         pattern\u001b[39m=\u001b[39;49mpattern,\n\u001b[0;32m   2558\u001b[0m         column_names\u001b[39m=\u001b[39;49mnormalized_column_names,\n\u001b[0;32m   2559\u001b[0m         transformations\u001b[39m=\u001b[39;49mtransformation_exps,\n\u001b[0;32m   2560\u001b[0m         copy_options\u001b[39m=\u001b[39;49mcopy_options,\n\u001b[0;32m   2561\u001b[0m         format_type_options\u001b[39m=\u001b[39;49mformat_type_options,\n\u001b[0;32m   2562\u001b[0m         validation_mode\u001b[39m=\u001b[39;49mvalidation_mode,\n\u001b[0;32m   2563\u001b[0m         user_schema\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49m_user_schema,\n\u001b[0;32m   2564\u001b[0m         cur_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49m_cur_options,\n\u001b[0;32m   2565\u001b[0m         create_table_from_infer_schema\u001b[39m=\u001b[39;49mcreate_table_from_infer_schema,\n\u001b[0;32m   2566\u001b[0m     ),\n\u001b[0;32m   2567\u001b[0m )\u001b[39m.\u001b[39;49m_internal_collect_with_tag_no_telemetry(statement_params\u001b[39m=\u001b[39;49mstatement_params)\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\dataframe.py:559\u001b[0m, in \u001b[0;36mDataFrame._internal_collect_with_tag_no_telemetry\u001b[1;34m(self, statement_params, block, data_type)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_internal_collect_with_tag_no_telemetry\u001b[39m(\n\u001b[0;32m    550\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    551\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[39m# we should always call this method instead of collect(), to make sure the\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[39m# query tag is set properly.\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    560\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plan,\n\u001b[0;32m    561\u001b[0m         block\u001b[39m=\u001b[39;49mblock,\n\u001b[0;32m    562\u001b[0m         data_type\u001b[39m=\u001b[39;49mdata_type,\n\u001b[0;32m    563\u001b[0m         _statement_params\u001b[39m=\u001b[39;49mcreate_or_update_statement_params_with_query_tag(\n\u001b[0;32m    564\u001b[0m             statement_params, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49mquery_tag, SKIP_LEVELS_THREE\n\u001b[0;32m    565\u001b[0m         ),\n\u001b[0;32m    566\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:417\u001b[0m, in \u001b[0;36mServerConnection.execute\u001b[1;34m(self, plan, to_pandas, to_iter, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39mif\u001b[39;00m is_in_stored_procedure() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsync query is not supported in stored procedure yet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m     )\n\u001b[1;32m--> 417\u001b[0m result_set, result_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_result_set(\n\u001b[0;32m    418\u001b[0m     plan, to_pandas, to_iter, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, block\u001b[39m=\u001b[39;49mblock, data_type\u001b[39m=\u001b[39;49mdata_type\n\u001b[0;32m    419\u001b[0m )\n\u001b[0;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    421\u001b[0m     \u001b[39mreturn\u001b[39;00m result_set\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:152\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     ne \u001b[39m=\u001b[39m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[0;32m    150\u001b[0m         e\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 152\u001b[0m     \u001b[39mraise\u001b[39;00m ne\u001b[39m.\u001b[39mwith_traceback(tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:85\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     84\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     86\u001b[0m     \u001b[39mexcept\u001b[39;00m snowflake\u001b[39m.\u001b[39mconnector\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mProgrammingError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     87\u001b[0m         tb \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:505\u001b[0m, in \u001b[0;36mServerConnection.get_result_set\u001b[1;34m(self, plan, to_pandas, to_iter, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39mfor\u001b[39;00m holder, id_ \u001b[39min\u001b[39;00m placeholders\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    504\u001b[0m     final_query \u001b[39m=\u001b[39m final_query\u001b[39m.\u001b[39mreplace(holder, id_)\n\u001b[1;32m--> 505\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_query(\n\u001b[0;32m    506\u001b[0m     final_query,\n\u001b[0;32m    507\u001b[0m     to_pandas,\n\u001b[0;32m    508\u001b[0m     to_iter \u001b[39mand\u001b[39;49;00m (i \u001b[39m==\u001b[39;49m \u001b[39mlen\u001b[39;49m(plan\u001b[39m.\u001b[39;49mqueries) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m),\n\u001b[0;32m    509\u001b[0m     is_ddl_on_temp_object\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mis_ddl_on_temp_object,\n\u001b[0;32m    510\u001b[0m     block\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_last,\n\u001b[0;32m    511\u001b[0m     data_type\u001b[39m=\u001b[39;49mdata_type,\n\u001b[0;32m    512\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    513\u001b[0m )\n\u001b[0;32m    514\u001b[0m placeholders[query\u001b[39m.\u001b[39mquery_id_place_holder] \u001b[39m=\u001b[39m (\n\u001b[0;32m    515\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_last \u001b[39melse\u001b[39;00m result\u001b[39m.\u001b[39mquery_id\n\u001b[0;32m    516\u001b[0m )\n\u001b[0;32m    517\u001b[0m result_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cursor\u001b[39m.\u001b[39mdescription\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:105\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m    102\u001b[0m         ex\u001b[39m.\u001b[39mcause\n\u001b[0;32m    103\u001b[0m     )\n\u001b[0;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mraise\u001b[39;00m ex\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:99\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[0;32m     98\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    100\u001b[0m \u001b[39mexcept\u001b[39;00m ReauthenticationRequest \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m    102\u001b[0m         ex\u001b[39m.\u001b[39mcause\n\u001b[0;32m    103\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:345\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[1;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m     query_id_log \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m [queryID: \u001b[39m\u001b[39m{\u001b[39;00mex\u001b[39m.\u001b[39msfqid\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(ex, \u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    344\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to execute query\u001b[39m\u001b[39m{\u001b[39;00mquery_id_log\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m     \u001b[39mraise\u001b[39;00m ex\n\u001b[0;32m    347\u001b[0m \u001b[39m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[39m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[39m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[39m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[39m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[39mif\u001b[39;00m block:\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:329\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[1;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_statement_params\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mSNOWPARK_SKIP_TXN_COMMIT_IN_DDL\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m--> 329\u001b[0m     results_cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cursor\u001b[39m.\u001b[39;49mexecute(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    330\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify_query_listeners(\n\u001b[0;32m    331\u001b[0m         QueryRecord(results_cursor\u001b[39m.\u001b[39msfqid, results_cursor\u001b[39m.\u001b[39mquery)\n\u001b[0;32m    332\u001b[0m     )\n\u001b[0;32m    333\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecute query [queryID: \u001b[39m\u001b[39m{\u001b[39;00mresults_cursor\u001b[39m.\u001b[39msfqid\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\connector\\cursor.py:804\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[1;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, file_stream)\u001b[0m\n\u001b[0;32m    800\u001b[0m     is_integrity_error \u001b[39m=\u001b[39m (\n\u001b[0;32m    801\u001b[0m         code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m100072\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    802\u001b[0m     )  \u001b[39m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[0;32m    803\u001b[0m     error_class \u001b[39m=\u001b[39m IntegrityError \u001b[39mif\u001b[39;00m is_integrity_error \u001b[39melse\u001b[39;00m ProgrammingError\n\u001b[1;32m--> 804\u001b[0m     Error\u001b[39m.\u001b[39;49merrorhandler_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection, \u001b[39mself\u001b[39;49m, error_class, errvalue)\n\u001b[0;32m    805\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\connector\\errors.py:276\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merrorhandler_wrapper\u001b[39m(\n\u001b[0;32m    255\u001b[0m     connection: SnowflakeConnection \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     error_value: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m \u001b[39mint\u001b[39m],\n\u001b[0;32m    259\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m     \u001b[39m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39m        exception to the first handler in that order.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     handed_over \u001b[39m=\u001b[39m Error\u001b[39m.\u001b[39;49mhand_to_other_handler(\n\u001b[0;32m    277\u001b[0m         connection,\n\u001b[0;32m    278\u001b[0m         cursor,\n\u001b[0;32m    279\u001b[0m         error_class,\n\u001b[0;32m    280\u001b[0m         error_value,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m handed_over:\n\u001b[0;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m Error\u001b[39m.\u001b[39merrorhandler_make_exception(\n\u001b[0;32m    284\u001b[0m             error_class,\n\u001b[0;32m    285\u001b[0m             error_value,\n\u001b[0;32m    286\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\connector\\errors.py:331\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m cursor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     cursor\u001b[39m.\u001b[39mmessages\u001b[39m.\u001b[39mappend((error_class, error_value))\n\u001b[1;32m--> 331\u001b[0m     cursor\u001b[39m.\u001b[39;49merrorhandler(connection, cursor, error_class, error_value)\n\u001b[0;32m    332\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39melif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\.conda\\envs\\env38\\lib\\site-packages\\snowflake\\connector\\errors.py:210\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_errorhandler\u001b[39m(\n\u001b[0;32m    194\u001b[0m     connection: SnowflakeConnection,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     error_value: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m],\n\u001b[0;32m    198\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39m\"\"\"Default error handler that raises an error.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39m        A Snowflake error.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(\n\u001b[0;32m    211\u001b[0m         msg\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmsg\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    212\u001b[0m         errno\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merrno\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    213\u001b[0m         sqlstate\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msqlstate\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    214\u001b[0m         sfqid\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    215\u001b[0m         done_format_msg\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdone_format_msg\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    216\u001b[0m         connection\u001b[39m=\u001b[39mconnection,\n\u001b[0;32m    217\u001b[0m         cursor\u001b[39m=\u001b[39mcursor,\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mSnowparkSQLException\u001b[0m: (1304): 100080 (22000): Number of columns in file (13) does not match that of the corresponding table (15), use file format option error_on_column_count_mismatch=false to ignore this error\n  File 'schema1/202102-citibike-tripdata.csv.gz', line 3, character 1\n  Row 1 starts at line 2, column \"RAW_SCHEMA1\"[\"USERTYPE\":13]\n  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client."
     ]
    }
   ],
   "source": [
    "csv_file_format_options = {\"FIELD_OPTIONALLY_ENCLOSED_BY\": \"'\\\"'\", \"skip_header\": 1}\n",
    "\n",
    "loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                     .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                     .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                     .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                     .option(\"NULL_IF\", \"NULL\")\\\n",
    "                     .option(\"pattern\", \"'.*2020.*[.]gz'\")\\\n",
    "                     .schema(load_schema1)\\\n",
    "                     .csv('@'+schema1_load_stage)\\\n",
    "                     .copy_into_table(state_dict['load_table_name']+str('schema1'), \n",
    "                                      format_type_options=csv_file_format_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load schema2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                     .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                     .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                     .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                     .option(\"NULL_IF\", \"NULL\")\\\n",
    "                     .option(\"pattern\", \"'.*2022.*[.]gz'\")\\\n",
    "                     .schema(load_schema2)\\\n",
    "                     .csv('@'+schema2_load_stage)\\\n",
    "                     .copy_into_table(state_dict['load_table_name']+str('schema2'), \n",
    "                                      format_type_options=csv_file_format_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transform:\n",
    "We have the raw data loaded. Now let's transform this data and clean it up. This will push the data to a final \\\"transformed\\\" table to be consumed by our Data Science team.  First we start by combining the two tables with the common columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_table_schema_names = [field.name for field in trips_table_schema.fields]\n",
    "transdf1 = session.table(state_dict['load_table_name']+'schema1')[trips_table_schema_names]\n",
    "transdf2 = session.table(state_dict['load_table_name']+'schema2')[trips_table_schema_names]\n",
    "transdf = transdf1.union_by_name(transdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different date formats \"2014-08-10 15:21:22\", \"1/1/2015 1:30\" and \"12/1/2014 02:04:53\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format_2 = \"1/1/2015 [0-9]:.*$\"      #1/1/2015 1:30 -> #M*M/D*D/YYYY H*H:M*M(:SS)*\n",
    "date_format_3 = \"1/1/2015 [0-9][0-9]:.*$\" #1/1/2015 10:30 -> #M*M/D*D/YYYY H*H:M*M(:SS)*\n",
    "date_format_4 = \"12/1/2014.*\"             #12/1/2014 02:04:53 -> M*M/D*D/YYYY \n",
    "\n",
    "#Change all dates to YYYY-MM-DD HH:MI:SS format\n",
    "date_format_match = \"^([0-9]?[0-9])/([0-9]?[0-9])/([0-9][0-9][0-9][0-9]) ([0-9]?[0-9]):([0-9][0-9])(:[0-9][0-9])?.*$\"\n",
    "date_format_repl = \"\\\\3-\\\\1-\\\\2 \\\\4:\\\\5\\\\6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "transdf.with_column('STARTTIME', F.regexp_replace(F.col('STARTTIME'),\n",
    "                                            F.lit(date_format_match), \n",
    "                                            F.lit(date_format_repl)))\\\n",
    "      .with_column('STARTTIME', F.to_timestamp('STARTTIME'))\\\n",
    "      .with_column('STOPTIME', F.regexp_replace(F.col('STOPTIME'),\n",
    "                                            F.lit(date_format_match), \n",
    "                                            F.lit(date_format_repl)))\\\n",
    "      .with_column('STOPTIME', F.to_timestamp('STOPTIME'))\\\n",
    "      .select(F.col('STARTTIME'), \n",
    "              F.col('STOPTIME'), \n",
    "              F.col('START_STATION_ID'), \n",
    "              F.col('START_STATION_NAME'), \n",
    "              F.col('START_STATION_LATITUDE'), \n",
    "              F.col('START_STATION_LONGITUDE'), \n",
    "              F.col('END_STATION_ID'), \n",
    "              F.col('END_STATION_NAME'), F.col('END_STATION_LATITUDE'), \n",
    "              F.col('END_STATION_LONGITUDE'), \n",
    "              F.col('USERTYPE'))\\\n",
    "      .write.mode('overwrite').save_as_table(state_dict['trips_table_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('STARTTIME', TimestampType(), nullable=True), StructField('STOPTIME', TimestampType(), nullable=True), StructField('START_STATION_ID', StringType(), nullable=True), StructField('START_STATION_NAME', StringType(), nullable=True), StructField('START_STATION_LATITUDE', StringType(), nullable=True), StructField('START_STATION_LONGITUDE', StringType(), nullable=True), StructField('END_STATION_ID', StringType(), nullable=True), StructField('END_STATION_NAME', StringType(), nullable=True), StructField('END_STATION_LATITUDE', StringType(), nullable=True), StructField('END_STATION_LONGITUDE', StringType(), nullable=True), StructField('USERTYPE', StringType(), nullable=True)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = session.table(state_dict['trips_table_name'])\n",
    "testdf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111397850"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Export code in functional modules for MLOps and orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dags/elt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dags/elt.py\n",
    "def schema1_definition():\n",
    "    from snowflake.snowpark import types as T\n",
    "    load_schema1 = T.StructType([T.StructField(\"TRIPDURATION\", T.StringType()),\n",
    "                             T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"BIKEID\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType()), \n",
    "                             T.StructField(\"BIRTH_YEAR\", T.StringType()),\n",
    "                             T.StructField(\"GENDER\", T.StringType())])\n",
    "    return load_schema1\n",
    "\n",
    "def schema2_definition():\n",
    "    from snowflake.snowpark import types as T\n",
    "    load_schema2 = T.StructType([T.StructField(\"ride_id\", T.StringType()), \n",
    "                             T.StructField(\"rideable_type\", T.StringType()), \n",
    "                             T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType())])\n",
    "    return load_schema2\n",
    "\n",
    "def conformed_schema():\n",
    "    from snowflake.snowpark import types as T\n",
    "    trips_table_schema = T.StructType([T.StructField(\"STARTTIME\", T.StringType()), \n",
    "                             T.StructField(\"STOPTIME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"START_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_NAME\", T.StringType()), \n",
    "                             T.StructField(\"END_STATION_ID\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"START_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LATITUDE\", T.StringType()),\n",
    "                             T.StructField(\"END_STATION_LONGITUDE\", T.StringType()),\n",
    "                             T.StructField(\"USERTYPE\", T.StringType())])\n",
    "    return trips_table_schema\n",
    "\n",
    "def extract_trips_to_stage(session, files_to_download: list, download_base_url: str, load_stage_name:str):\n",
    "    import os \n",
    "    import requests\n",
    "    from zipfile import ZipFile\n",
    "    import gzip\n",
    "    from datetime import datetime\n",
    "    from io import BytesIO\n",
    "    \n",
    "    schema1_download_files = list()\n",
    "    schema2_download_files = list()\n",
    "    schema2_start_date = datetime.strptime('202102', \"%Y%m\")\n",
    "\n",
    "    for file_name in files_to_download:\n",
    "        file_start_date = datetime.strptime(file_name.split(\"-\")[0], \"%Y%m\")\n",
    "        if file_start_date < schema2_start_date:\n",
    "            schema1_download_files.append(file_name)\n",
    "        else:\n",
    "            schema2_download_files.append(file_name)\n",
    "         \n",
    "        \n",
    "    schema1_load_stage = load_stage_name+'/schema1/'\n",
    "    schema1_files_to_load = list()\n",
    "    for zip_file_name in schema1_download_files:\n",
    "\n",
    "        url = download_base_url+zip_file_name\n",
    "\n",
    "        print('Downloading and unzipping: '+url)\n",
    "        r = requests.get(url)\n",
    "        file = ZipFile(BytesIO(r.content))\n",
    "        csv_file_name=file.namelist()[0]\n",
    "        file.extract(csv_file_name)\n",
    "        file.close()\n",
    "\n",
    "        print('Putting '+csv_file_name+' to stage: '+schema1_load_stage)\n",
    "        session.file.put(local_file_name=csv_file_name, \n",
    "                         stage_location=schema1_load_stage, \n",
    "                         source_compression='NONE', \n",
    "                         overwrite=True)\n",
    "        schema1_files_to_load.append(csv_file_name)\n",
    "        os.remove(csv_file_name)\n",
    "\n",
    "        \n",
    "    schema2_load_stage = load_stage_name+'/schema2/'\n",
    "    schema2_files_to_load = list()\n",
    "    for zip_file_name in schema2_download_files:\n",
    "\n",
    "        url = download_base_url+zip_file_name\n",
    "\n",
    "        print('Downloading and unzipping: '+url)\n",
    "        r = requests.get(url)\n",
    "        file = ZipFile(BytesIO(r.content))\n",
    "        csv_file_name=file.namelist()[0]\n",
    "        file.extract(csv_file_name)\n",
    "        file.close()\n",
    "\n",
    "        print('Putting '+csv_file_name+' to stage: '+schema2_load_stage)\n",
    "        session.file.put(local_file_name=csv_file_name, \n",
    "                         stage_location=schema2_load_stage, \n",
    "                         source_compression='NONE', \n",
    "                         overwrite=True)\n",
    "        schema2_files_to_load.append(csv_file_name)\n",
    "        os.remove(csv_file_name)\n",
    "        \n",
    "    load_stage_names = {'schema1' : schema1_load_stage, 'schema2' : schema2_load_stage}\n",
    "    files_to_load = {'schema1': schema1_files_to_load, 'schema2': schema2_files_to_load}\n",
    "\n",
    "    return load_stage_names, files_to_load\n",
    "    \n",
    "def load_trips_to_raw(session, files_to_load:dict, load_stage_names:dict, load_table_name:str):\n",
    "    from snowflake.snowpark import functions as F\n",
    "    from snowflake.snowpark import types as T\n",
    "    from datetime import datetime\n",
    "    \n",
    "    csv_file_format_options = {\"FIELD_OPTIONALLY_ENCLOSED_BY\": \"'\\\"'\", \"skip_header\": 1}\n",
    "\n",
    "    if len(files_to_load['schema1']) > 0:\n",
    "        load_schema1 = schema1_definition()\n",
    "        loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                         .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                         .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                         .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                         .option(\"NULL_IF\", \"NULL\")\\\n",
    "                         .schema(load_schema1)\\\n",
    "                         .csv('@'+load_stage_names['schema1'])\\\n",
    "                         .copy_into_table(load_table_name+'schema1', \n",
    "                                          files=files_to_load['schema1'],\n",
    "                                          format_type_options=csv_file_format_options)\n",
    "                              \n",
    "    if len(files_to_load['schema2']) > 0:\n",
    "        load_schema2 = schema2_definition()\n",
    "        loaddf = session.read.option(\"SKIP_HEADER\", 1)\\\n",
    "                         .option(\"FIELD_OPTIONALLY_ENCLOSED_BY\", \"\\042\")\\\n",
    "                         .option(\"COMPRESSION\", \"GZIP\")\\\n",
    "                         .option(\"NULL_IF\", \"\\\\\\\\N\")\\\n",
    "                         .option(\"NULL_IF\", \"NULL\")\\\n",
    "                         .schema(load_schema2)\\\n",
    "                         .csv('@'+load_stage_names['schema2'])\\\n",
    "                         .copy_into_table(load_table_name+'schema2', \n",
    "                                          files=files_to_load['schema2'],\n",
    "                                          format_type_options=csv_file_format_options)\n",
    "        \n",
    "    load_table_names = {'schema1' : load_table_name+str('schema1'), \n",
    "                         'schema2' : load_table_name+str('schema2')}\n",
    "                         \n",
    "    return load_table_names\n",
    "    \n",
    "def transform_trips(session, stage_table_names:dict, trips_table_name:str):\n",
    "    from snowflake.snowpark import functions as F\n",
    "        \n",
    "    #Change all dates to YYYY-MM-DD HH:MI:SS format\n",
    "    date_format_match = \"^([0-9]?[0-9])/([0-9]?[0-9])/([0-9][0-9][0-9][0-9]) ([0-9]?[0-9]):([0-9][0-9])(:[0-9][0-9])?.*$\"\n",
    "    date_format_repl = \"\\\\3-\\\\1-\\\\2 \\\\4:\\\\5\\\\6\"\n",
    "\n",
    "    trips_table_schema = conformed_schema()\n",
    "                         \n",
    "    trips_table_schema_names = [field.name for field in trips_table_schema.fields]\n",
    "                         \n",
    "    transdf1 = session.table(stage_table_names['schema1'])[trips_table_schema_names]\n",
    "    transdf2 = session.table(stage_table_names['schema2'])[trips_table_schema_names]\n",
    "                         \n",
    "    transdf = transdf1.union_by_name(transdf2)\\\n",
    "                      .with_column('STARTTIME', F.regexp_replace(F.col('STARTTIME'),\n",
    "                                                                F.lit(date_format_match), \n",
    "                                                                F.lit(date_format_repl)))\\\n",
    "                      .with_column('STARTTIME', F.to_timestamp('STARTTIME'))\\\n",
    "                      .with_column('STOPTIME', F.regexp_replace(F.col('STOPTIME'),\n",
    "                                                               F.lit(date_format_match), \n",
    "                                                               F.lit(date_format_repl)))\\\n",
    "                      .with_column('STOPTIME', F.to_timestamp('STOPTIME'))\\\n",
    "                      .write.mode('overwrite').save_as_table(trips_table_name)\n",
    "\n",
    "    return trips_table_name\n",
    "\n",
    "def reset_database(session, state_dict:dict, prestaged=False):\n",
    "    _ = session.sql('CREATE OR REPLACE DATABASE '+state_dict['connection_parameters']['database']).collect()\n",
    "    _ = session.sql('CREATE SCHEMA '+state_dict['connection_parameters']['schema']).collect() \n",
    "\n",
    "    if prestaged:\n",
    "        sql_cmd = 'CREATE OR REPLACE STAGE '+state_dict['load_stage_name']+\\\n",
    "                  ' url='+state_dict['connection_parameters']['download_base_url']\n",
    "        _ = session.sql(sql_cmd).collect()\n",
    "    else: \n",
    "        _ = session.sql('CREATE STAGE IF NOT EXISTS '+state_dict['load_stage_name']).collect()\n",
    "\n",
    "    load_schema1=schema1_definition()\n",
    "    session.create_dataframe([[None]*len(load_schema1.names)], schema=load_schema1)\\\n",
    "           .na.drop()\\\n",
    "           .write\\\n",
    "           .save_as_table(state_dict['load_table_name']+'schema1')\n",
    "\n",
    "    load_schema2=schema2_definition()\n",
    "    session.create_dataframe([[None]*len(load_schema2.names)], schema=load_schema2)\\\n",
    "           .na.drop()\\\n",
    "           .write\\\n",
    "           .save_as_table(state_dict['load_table_name']+'schema2')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "msauthor": "trbye",
  "vscode": {
   "interpreter": {
    "hash": "73d34bfba256746ec607fb96d130988eff5c98f2b6f522aea7687ed77177669f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
